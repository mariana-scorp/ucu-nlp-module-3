{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nata/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import stanza\n",
    "import itertools\n",
    "import requests\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "\n",
    "#stanza.download(\"uk\")\n",
    "#stanza.download(\"en\")\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "def get_all_edges(concept):\n",
    "    offset = 0\n",
    "    req = requests.get('http://api.conceptnet.io/c/en/' + concept + '?offset=' + str(offset) + '&limit=1000').json()\n",
    "    all_edges = req\n",
    "    while len(req['edges']) == 1000:\n",
    "        offset += 1000\n",
    "        req = requests.get('http://api.conceptnet.io/c/en/' + concept + '?offset=' + str(offset) + '&limit=1000').json()\n",
    "        all_edges['edges'] += req['edges']\n",
    "    return all_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [\"the\", \"a\", \"an\", '.',',','!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from https://nlp.stanford.edu/projects/snli/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"snli_1.0/snli_1.0/snli_1.0_train.jsonl\"\n",
    "test_path = \"snli_1.0/snli_1.0/snli_1.0_test.jsonl\"\n",
    "dev_path = \"snli_1.0/snli_1.0/snli_1.0_dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file):    \n",
    "    dict_list = []\n",
    "    with open(file, \"r\") as read_file:\n",
    "        for line in read_file.readlines():\n",
    "            data = json.loads(line)\n",
    "            if data[\"gold_label\"]!='-':\n",
    "                dict_list.append(data)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features\n",
    "\n",
    "#### My progress:\n",
    "\n",
    "**I iteration** : додала перші ж ознаки, які прийшли на думку, щоб мати певний мінімальний бенчмарк\n",
    "1. кількість спільних слів у першому та другому реченні (n_common)\n",
    "2. кількість слів у першому реченні (n_s1)\n",
    "3. кількість слів у другому реченні (n_s2)\n",
    "\n",
    "Я одразу ж отримала 0.47 accuracy на *dev* даних, що доволі непогано, порівнюючи з рандомним класифікатором, що дає 0.33 accuracy\n",
    "\n",
    "**II iteration** : ознаки лексичної схожості. Спочатку пробувала використати лематизацію *stanza*. Коли я пробувала обробити кожне речення окремо, то він обробляв *test* вибірку більше 10 хв і не завершив обробку. Коли я пробувала обробити всі речення одночасно, то він ви'їв 10гб оперативки і аналогічно не закінчив обробку. Тому я вирішила перейти на щось легше і зупинилась на SnowballStemmer. Нові ознаки:\n",
    "1. кількість спільних stemmed-слів(bigrams,trigrams) у першому та другому реченні (n_common_...)\n",
    "2. кількість stemmed-слів(bigrams,trigrams) у першому реченні (n_1_...)\n",
    "3. кількість stemmed-слів(bigrams,trigrams) у другому реченні (n_2_...)\n",
    "4. частка спільних унікальних stemmed-слів(bigrams,trigrams) у першому та другому реченні з-поміж всіх унікальних stemmed-слів(bigrams,trigrams) (per_common_...)\n",
    "5. частка stemmed-слів (bigrams,trigrams) у першому реченні з сумарних stemmed-слів (bigrams,trigrams) (per_1_...)\n",
    "6. частка унікальних stemmed-слів (bigrams,trigrams) у першому реченні з сумарних унікальних stemmed-слів (bigrams,trigrams) (per_set_1...)\n",
    "7. чи є слово \"not\" у першому реченні\n",
    "8. чи є слово \"not\" у друг реченні\n",
    "\n",
    "Я отримала 0.49 accuracy на *dev* даних\n",
    "\n",
    "**III iteration** : ознаки граматичної схожості. Оскільки в даних вже є parse tree з розписаними частинами мови, я використала його. Нові ознаки:\n",
    "1. Частка певної частини мови у кожному реченні: іменник, дієслово, прийменник, займенник (per_...)\n",
    "2. Частка одинакових слів у двох реченнях з-поміж певної частини мови: іменник, дієслово, прийменник, чисельник (per_common_...)\n",
    "\n",
    "Отримала 0.52 accuracy на *dev* даних.\n",
    "\n",
    "**IV iteration** : ознаки семантичної схожості. Я хотіла в даному випадку використати ConceptNet і знайти всі можливі зв'язки і порахувати їх. Проте використати ConceptNet можна лише через запити, тому це займає дуже довго. В результаті я обмежилась лише пошуком синонімів (інших зв'язків там немає) за допомогою WordNet. Нижче є приклад використання ConceptNet і WordNet і час відпрацювання. звісно, щоб досягнути швидкості обробки, я втратила можливіть аналізувати більше зв'язків і точність впала. Нові ознаки:\n",
    "1. Кількість синонімів у між двома реченнями (n_syn)\n",
    "\n",
    "Отримала 0.52 accuracy на *dev* даних. (точність не змінилась)\n",
    "\n",
    "Фінальні результати див нижче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma', tokenize_no_ssplit=True)\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lex_features(s1,s2,name):\n",
    "    f_names = {}\n",
    "    f_names[\"n_common_\"+name] = len(set(s1).intersection(set(s2)))\n",
    "    f_names[\"n_1_\"+name] = len(s1)\n",
    "    f_names[\"n_2_\"+name] = len(s2)\n",
    "    if len(set(s1+s2))!=0:\n",
    "        f_names[\"per_common_\"+name] = len(set(s1).intersection(set(s2)))/len(set(s1+s2))\n",
    "        f_names[\"per_1_\"+name] = len(s1)/len(s1+s2)\n",
    "        f_names[\"per_set_1_\"+name] = len(set(s1))/len(set(s1+s2))\n",
    "        #f_names[\"per_2_\"+name] = len(s2)/len(s1+s2)   = 1 - per_1_\n",
    "        #f_names[\"per_set_2_\"+name] = len(set(s2))/len(set(s1+s2))    = 1 - per_set_1_\n",
    "    else:\n",
    "        f_names[\"per_common_\"+name] = 0\n",
    "        f_names[\"per_1_\"+name] = 0\n",
    "        f_names[\"per_set_1_\"+name] = 0\n",
    "    return f_names\n",
    "\n",
    "def poc_intersection(s1,s2,POS):\n",
    "    pos1 = re.findall(f\"(\\({POS}[^\\(^\\)]*\\))\", s1)\n",
    "    pos2 = re.findall(f\"(\\({POS}[^\\(^\\)]*\\))\", s2)\n",
    "    pos_words1 = [pair.split(\" \")[1][:-1] for pair in pos1]\n",
    "    pos_words2 = [pair.split(\" \")[1][:-1] for pair in pos2]\n",
    "    pos_words_stemmed1 = [englishStemmer.stem(word.lower()) for word in pos_words1]\n",
    "    pos_words_stemmed2 = [englishStemmer.stem(word.lower()) for word in pos_words2]\n",
    "    if len(set(pos_words_stemmed1+pos_words_stemmed2)) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(set(pos_words_stemmed1).intersection(set(pos_words_stemmed2)))/len(set(pos_words_stemmed1+pos_words_stemmed2))\n",
    "    \n",
    "def count_relations(s1,s2):\n",
    "    relations = []\n",
    "    for pair in list(itertools.product(s1, s2)):\n",
    "        if pair[0]==pair[1]:\n",
    "            continue\n",
    "        all_concepts = get_all_edges(pair[0])\n",
    "        for concept in all_concepts[\"edges\"]:\n",
    "            if concept[\"end\"][\"label\"] == pair[1]:\n",
    "                relations.append(concept[\"rel\"][\"label\"])\n",
    "    r,c = np.unique(relations, return_counts = True)\n",
    "    relation_count_dict = {}\n",
    "    for i in range(len(r)):\n",
    "        relation_count_dict[r[i]] = c[i]\n",
    "    return relation_count_dict\n",
    "\n",
    "def count_synonyms(s1,s2):\n",
    "    c = 0\n",
    "    for word in s1:\n",
    "        syns = wordnet.synsets(word) \n",
    "        synonims = set([s.lemmas()[0].name() for s in syns])\n",
    "        synonims = synonims - set([word])\n",
    "        if len(synonims.intersection(set(s2)))>0:\n",
    "            c = c + 1\n",
    "    return c\n",
    "\n",
    "def get_features(d):\n",
    "    s_token1 = [t.lower() for t in d[\"sentence1\"].split(\" \")]\n",
    "    s_token2 = [t.lower() for t in d[\"sentence2\"].split(\" \")]\n",
    "    s1 = list(set(s_token1) - set(my_stop_words))\n",
    "    s2 = list(set(s_token2) - set(my_stop_words))\n",
    "    \n",
    "    stems1 = [englishStemmer.stem(token) for token in s1]\n",
    "    stems2 = [englishStemmer.stem(token) for token in s2]\n",
    "    \n",
    "    two_grams1 = [\" \".join(g) for g in ngrams(stems1,2)]\n",
    "    two_grams2 = [\" \".join(g) for g in ngrams(stems2,2)]\n",
    "    \n",
    "    three_grams1 = [\" \".join(g) for g in ngrams(stems1,3)]\n",
    "    three_grams2 = [\" \".join(g) for g in ngrams(stems2,3)]\n",
    "    \n",
    "    f_names = {}\n",
    "    \n",
    "    # базові ознаки \n",
    "    f_names[\"n_common\"] = len(set(s1).intersection(set(s2)))\n",
    "    f_names[\"n_s1\"] = len(s1)\n",
    "    f_names[\"n_s2\"] = len(s2)\n",
    "    \n",
    "    # ознаки лексичної схожості\n",
    "    l_f = lex_features(stems1,stems2,\"stem\")\n",
    "    l_f2 = lex_features(two_grams1,two_grams2,\"two_grams\")\n",
    "    l_f3 = lex_features(three_grams1,three_grams2,\"three_grams\")\n",
    "    f_names = {**f_names, **l_f, **l_f2, **l_f3}\n",
    "    \n",
    "    f_names[\"is_not_stems1\"] = \"not\" in stems1\n",
    "    f_names[\"is_not_stems2\"] = \"not\" in stems2\n",
    "    \n",
    "    # ознаки граматичної схожості\n",
    "    f_names[\"per_NN1\"] = (d[\"sentence1_parse\"].count(\"NN \")+d[\"sentence1_parse\"].count(\"NNP \")+d[\"sentence1_parse\"].count(\"NNS \"))/len(s_token1)\n",
    "    f_names[\"per_NN2\"] = (d[\"sentence2_parse\"].count(\"NN \")+d[\"sentence2_parse\"].count(\"NNP \")+d[\"sentence2_parse\"].count(\"NNS \"))/len(s_token2)\n",
    "    f_names[\"per_VB1\"] = (d[\"sentence1_parse\"].count(\"VB\"))/len(s_token1)\n",
    "    f_names[\"per_VB2\"] = (d[\"sentence2_parse\"].count(\"VB\"))/len(s_token2)\n",
    "    f_names[\"per_JJ1\"] = (d[\"sentence1_parse\"].count(\"JJ\"))/len(s_token1)\n",
    "    f_names[\"per_JJ2\"] = (d[\"sentence2_parse\"].count(\"JJ\"))/len(s_token2)\n",
    "    f_names[\"per_PRP1\"] = (d[\"sentence1_parse\"].count(\"PRP\"))/len(s_token1)\n",
    "    f_names[\"per_PRP2\"] = (d[\"sentence2_parse\"].count(\"PRP\"))/len(s_token2)\n",
    "    f_names[\"per_common_NN\"] = poc_intersection(d[\"sentence1_parse\"],d[\"sentence2_parse\"],\"NN\")\n",
    "    f_names[\"per_common_VB\"] = poc_intersection(d[\"sentence1_parse\"],d[\"sentence2_parse\"],\"VB\")\n",
    "    f_names[\"per_common_CD\"] = poc_intersection(d[\"sentence1_parse\"],d[\"sentence2_parse\"],\"CD\")\n",
    "    f_names[\"per_common_JJ\"] = poc_intersection(d[\"sentence1_parse\"],d[\"sentence2_parse\"],\"JJ\")\n",
    "    \n",
    "    # ознаки семантичної схожості\n",
    "    f_names[\"n_syn\"] = count_synonyms(s1,s2)\n",
    "    \n",
    "    return f_names.values(), f_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        f, f_names = get_features(d)\n",
    "        X.append(f)\n",
    "        y.append(d[\"gold_label\"])\n",
    "    return pd.DataFrame(X, columns = f_names), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.42 s, sys: 155 ms, total: 2.57 s\n",
      "Wall time: 54.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Synonym': 2}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### Example of using ConceptNet and time \n",
    "c = count_relations(['Two','women','are','embracing','while','holding','to','go','packages'],\n",
    "                    ['The','sisters','are','hugging','goodbye','while','holding','to','go','packages'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 3.88 ms, total: 15.3 ms\n",
      "Wall time: 14.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### Example of using WordNet and time \n",
    "count_synonyms(['Two','women','are','embracing','while','holding','to','go','packages'],\n",
    "              ['The','sisters','are','hugging','goodbye','while','holding','to','go','packages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST shape: (9824, 36) (9824,)\n",
      "TRAIN shape: (549367, 36) (549367,)\n",
      "TRAIN shape: (9842, 36) (9842,)\n",
      "CPU times: user 6min 22s, sys: 3.7 s, total: 6min 26s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_test = parse_data(test_path)\n",
    "X_test, y_test = get_X_y(data_test)\n",
    "print(\"TEST shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "data_train = parse_data(train_path)\n",
    "X_train, y_train = get_X_y(data_train)\n",
    "print(\"TRAIN shape:\", X_train.shape, y_train.shape)\n",
    "\n",
    "data_dev = parse_data(dev_path)\n",
    "X_dev, y_dev = get_X_y(data_dev)\n",
    "print(\"TRAIN shape:\", X_dev.shape, y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 36 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['n_common', 'n_s1', 'n_s2', 'n_common_stem', 'n_1_stem', 'n_2_stem',\n",
       "       'per_common_stem', 'per_1_stem', 'per_set_1_stem', 'n_common_two_grams',\n",
       "       'n_1_two_grams', 'n_2_two_grams', 'per_common_two_grams',\n",
       "       'per_1_two_grams', 'per_set_1_two_grams', 'n_common_three_grams',\n",
       "       'n_1_three_grams', 'n_2_three_grams', 'per_common_three_grams',\n",
       "       'per_1_three_grams', 'per_set_1_three_grams', 'is_not_stems1',\n",
       "       'is_not_stems2', 'per_NN1', 'per_NN2', 'per_VB1', 'per_VB2', 'per_JJ1',\n",
       "       'per_JJ2', 'per_PRP1', 'per_PRP2', 'per_common_NN', 'per_common_VB',\n",
       "       'per_common_CD', 'per_common_JJ', 'n_syn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total {len(X_train.columns)} features.\")\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nata/PycharmProjects/UCU/NLP/nlp_env/lib/python3.8/site-packages/sklearn/dummy.py:131: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  warnings.warn(\"The default value of strategy will change from \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.33      0.33      0.33      3278\n",
      "   entailment       0.35      0.35      0.35      3329\n",
      "      neutral       0.32      0.33      0.32      3235\n",
      "\n",
      "     accuracy                           0.34      9842\n",
      "    macro avg       0.34      0.34      0.34      9842\n",
      " weighted avg       0.34      0.34      0.34      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train,y_train)\n",
    "dev_preds_dummy = dummy.predict(X_dev)\n",
    "print(classification_report(y_dev, dev_preds_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nata/PycharmProjects/UCU/NLP/nlp_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for DEV\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.51      0.50      3278\n",
      "   entailment       0.57      0.62      0.60      3329\n",
      "      neutral       0.52      0.44      0.47      3235\n",
      "\n",
      "     accuracy                           0.52      9842\n",
      "    macro avg       0.52      0.52      0.52      9842\n",
      " weighted avg       0.52      0.52      0.52      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train,y_train)\n",
    "dev_preds = pipe.predict(X_dev)\n",
    "train_preds = pipe.predict(X_train)\n",
    "print(\"Metrics for DEV\")\n",
    "print(classification_report(y_dev, dev_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for TRAIN\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.52      0.50    183187\n",
      "   entailment       0.55      0.61      0.58    183416\n",
      "      neutral       0.52      0.43      0.47    182764\n",
      "\n",
      "     accuracy                           0.52    549367\n",
      "    macro avg       0.52      0.52      0.52    549367\n",
      " weighted avg       0.52      0.52      0.52    549367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics for TRAIN\")\n",
    "print(classification_report(y_train, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.005658797375599877, 'is_not_stems1'),\n",
       " (0.038050000149370745, 'per_PRP1'),\n",
       " (0.059917332447525, 'per_VB1'),\n",
       " (0.060038447359466035, 'n_1_stem'),\n",
       " (0.060038447359466035, 'n_1_two_grams'),\n",
       " (0.060038447359466035, 'n_s1'),\n",
       " (0.06087236740131889, 'per_JJ1'),\n",
       " (0.0634655906730535, 'per_common_CD'),\n",
       " (0.0649716111478523, 'n_1_three_grams'),\n",
       " (0.09073235871119063, 'n_syn'),\n",
       " (0.0965806144060059, 'per_VB2'),\n",
       " (0.1202532584956459, 'is_not_stems2'),\n",
       " (0.14764025798170324, 'n_common_two_grams'),\n",
       " (0.1536043883507013, 'n_2_three_grams'),\n",
       " (0.18881596170279294, 'per_NN1'),\n",
       " (0.22998978708004114, 'n_2_stem'),\n",
       " (0.22998978708004114, 'n_s2'),\n",
       " (0.23297933770477736, 'n_2_two_grams'),\n",
       " (0.23974664493350847, 'per_PRP2'),\n",
       " (0.25194613424760004, 'per_NN2'),\n",
       " (0.2538109694704606, 'n_common_three_grams'),\n",
       " (0.2646084799407681, 'per_common_JJ'),\n",
       " (0.27344682934027537, 'per_common_three_grams'),\n",
       " (0.33703664049393073, 'per_common_VB'),\n",
       " (0.34066395040502484, 'per_JJ2'),\n",
       " (0.7012419183538059, 'per_1_three_grams'),\n",
       " (0.8251869479118275, 'per_common_NN'),\n",
       " (0.8910206510802267, 'per_common_two_grams'),\n",
       " (1.1075920223868658, 'n_common'),\n",
       " (1.383270735164411, 'n_common_stem'),\n",
       " (1.430321891315441, 'per_set_1_three_grams'),\n",
       " (1.7129772806837404, 'per_set_1_two_grams'),\n",
       " (1.7174372360868506, 'per_common_stem'),\n",
       " (1.767299870704945, 'per_1_two_grams'),\n",
       " (2.362154707738412, 'per_set_1_stem'),\n",
       " (2.9669102057616517, 'per_1_stem')]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Feature importance\n",
    "sorted(list(zip(np.absolute(pipe.named_steps[\"logisticregression\"].coef_).sum(axis = 0), X_dev.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for TEST\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.52      0.50      3237\n",
      "   entailment       0.58      0.61      0.60      3368\n",
      "      neutral       0.51      0.45      0.48      3219\n",
      "\n",
      "     accuracy                           0.53      9824\n",
      "    macro avg       0.53      0.53      0.53      9824\n",
      " weighted avg       0.53      0.53      0.53      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = pipe.predict(X_test)\n",
    "print(\"Metrics for TEST\")\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фінальна точність на *test* даних рівна 0.53.\n",
    "\n",
    "**Висновок:**\n",
    "Загалом я змогла покращити точність за допомогою лінгвіністичних ознак схожості, проте є ще варіанти до покращення:\n",
    "\n",
    "1. використати більше ознак семантичної схожості. Наприклад, AllenNLP точно має PythonAPI, проте не експериментувала, тому не знаю наскільки він швидкий\n",
    "2. використати Text Similarity, наприклад відстань Левенштейна \n",
    "3. використати Word2Vec представлення даних (ае це не в рамках цього модуля)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
