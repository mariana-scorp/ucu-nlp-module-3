# Кроки виконання ДЗ

* Огляд даних. Зробив логування роботи оракула. Тепер краще розумію як все працює.
  Ідеї по покращенню:
    * Розібратися з непроекційними деревами - вибрати найменше. Як перескаються звязки?
    * Багато знаків пунктуації можливо вчити модель без них?
    * Отут https://web.stanford.edu/~jurafsky/slp3/15.pdf пропонують простіший алгоритм без кроку Reduce(arc-standart) та пропонують різні сети параметрів для моделі
    * Тюнити гімерпараметри, міняти алгоритм - просто і довго.
* Зі відношеннями між словми що пересікаються все ясно. [Ось тут](https://www.diva-portal.org/smash/get/diva2:1221345/FULLTEXT01.pdf)  описано як реалізувати SWAP - виглядає просто.
* Переніс оракул як змінну, щоб робити інші оракули. Виявилося не так просто SWAP крок зробити, більшість описують SWAP для алгоритмів arc-standart.
* Спробую поміняти модель. [В прикладі nltk](https://www.nltk.org/_modules/nltk/parse/transitionparser.html) використовується SVM. Будувалася модель порівняно довше, відпрацьовуй значно гірше логістичного регресії.
* Спробував класифікувати типи залежностей - пішов простішим шляхом - вирішив розширити класи. Модель будувалася довше. Загалом для класів з високою частотою ситуація непогана ситуація непогана. Як мінімум можна спробувати без підтипу натренувати модель, або тренувати на частотких класах, щоб модель не "розпорошувалася" на несуттєві класи.

P.S. Зробив я більше ніж дві ітерації, але суттєвого покращення я не добився. Загалом тема модулю мені зрозуміла, але власне сама домашка заходила досить важко))